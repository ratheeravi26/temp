{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks Notebook: Ingest Microsoft 365 Group Owners and Maintain SCD Type 2 History (Delta + ADLS Gen2)\n",
        "\n",
        "This notebook fetches the current owners for all active Microsoft 365 groups (using the Microsoft Graph API), and persists the ownership history to a Delta table using SCD Type 2 semantics. The pipeline is **idempotent**: re-running the notebook for the same `run_timestamp` will produce the same final state.\n",
        "\n",
        "### How to use\n",
        "1. Configure the Databricks widgets at the top of the notebook (or set them before running as a job).  \n",
        "2. Ensure the secret scope referenced by `key_vault_scope` contains the keys: `m365_tenant_id`, `m365_client_id`, `m365_client_secret`.  \n",
        "3. Run the notebook on a cluster with network access to Microsoft Graph and your ADLS Gen2 storage account.  \n",
        "\n",
        "### Key principles implemented\n",
        "- **Idempotency** via single `run_timestamp` and stateful comparison to `is_current = true` records.  \n",
        "- **Atomicity** by using a single Delta `MERGE`.  \n",
        "- **Resilience** through pagination and exponential-backoff retries when calling Graph API.  \n",
        "- **Scalability** by parallelizing API calls using `mapInPandas`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup & Configuration\n",
        "\n",
        "Create Databricks widgets for parameterization. We capture a single `run_timestamp` (UTC) at the start of the notebook; this value will be used consistently for all SCD `start_date` / `end_date` assignments to guarantee idempotency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Widgets (parameterize path and secret scope; do not hardcode values)\n",
        "dbutils.widgets.text(\"source_group_table_path\", \"abfss://bronze@your_storage_account.dfs.core.windows.net/m365/groups/\", \"Source group table path (ABFSS)\")\n",
        "dbutils.widgets.text(\"target_owner_table_path\", \"abfss://silver@your_storage_account.dfs.core.windows.net/m365/group_owners_history/\", \"Target owner table path (ABFSS)\")\n",
        "dbutils.widgets.text(\"key_vault_scope\", \"my_keyvault_scope\", \"Databricks secret scope for AAD app credentials\")\n",
        "\n",
        "source_group_table_path = dbutils.widgets.get(\"source_group_table_path\")\n",
        "target_owner_table_path = dbutils.widgets.get(\"target_owner_table_path\")\n",
        "key_vault_scope = dbutils.widgets.get(\"key_vault_scope\")\n",
        "\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# Single consistent run timestamp for this execution (UTC). This ensures idempotency.\n",
        "run_timestamp = datetime.now(timezone.utc).replace(microsecond=0)\n",
        "run_timestamp_str = run_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f\"run_timestamp (UTC): {run_timestamp_str}\")\n",
        "\n",
        "print(f\"Source path: {source_group_table_path}\")\n",
        "print(f\"Target path: {target_owner_table_path}\")\n",
        "print(f\"Secret scope: {key_vault_scope}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Dependencies\n",
        "\n",
        "Install required Python packages. If your cluster already has these installed, this is safe — `%pip` will be a no-op or upgrade. If `%pip` is restricted, install libraries at the cluster level or via an init script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install msal and requests if not present\n",
        "%pip install msal requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Imports\n",
        "\n",
        "Import PySpark and other helpers used throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType\n",
        "from delta.tables import DeltaTable\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import msal\n",
        "import pyspark\n",
        "print(\"Imports OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Retrieve AAD App Credentials (from Databricks secret scope)\n",
        "\n",
        "Expect the following secrets inside the `key_vault_scope`:\n",
        "- `m365_tenant_id`\n",
        "- `m365_client_id`\n",
        "- `m365_client_secret`\n",
        "\n",
        "This prevents hardcoding secrets in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve secrets\n",
        "tenant_id = dbutils.secrets.get(scope=key_vault_scope, key=\"m365_tenant_id\")\n",
        "client_id = dbutils.secrets.get(scope=key_vault_scope, key=\"m365_client_id\")\n",
        "client_secret = dbutils.secrets.get(scope=key_vault_scope, key=\"m365_client_secret\")\n",
        "\n",
        "if not (tenant_id and client_id and client_secret):\n",
        "    raise Exception(\"Missing one or more AAD app credentials in the secret scope.\")\n",
        "\n",
        "print(\"Successfully retrieved AAD app credentials from secret scope (not showing values).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Authentication helper (MSAL)\n",
        "\n",
        "We use the client credentials (service principal) flow to obtain an access token for Microsoft Graph (`https://graph.microsoft.com/.default`). The token is broadcast to executors for use by `mapInPandas` workers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUTHORITY = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
        "SCOPE = [\"https://graph.microsoft.com/.default\"]\n",
        "\n",
        "app = msal.ConfidentialClientApplication(\n",
        "    client_id,\n",
        "    authority=AUTHORITY,\n",
        "    client_credential=client_secret,\n",
        ")\n",
        "\n",
        "# Acquire token at driver\n",
        "token_result = app.acquire_token_for_client(scopes=SCOPE)\n",
        "if \"access_token\" not in token_result:\n",
        "    raise Exception(f\"Failed to acquire access token: {token_result}\")\n",
        "access_token = token_result[\"access_token\"]\n",
        "print(\"Successfully acquired access token (not shown).\")\n",
        "\n",
        "# Broadcast token to workers\n",
        "access_token_b = spark.sparkContext.broadcast(access_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Read source groups table and prepare group id list\n",
        "\n",
        "Read the `m365_groups_delta` Delta table from the supplied path. Filter for active groups using common heuristics. Only the `id` column (aliased to `group_id`) is selected for subsequent Graph API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    groups_df = spark.read.format(\"delta\").load(source_group_table_path)\n",
        "except Exception as e:\n",
        "    raise Exception(f\"Failed to read source group table at {source_group_table_path}: {e}\")\n",
        "\n",
        "print(f\"Source groups columns: {groups_df.columns}\")\n",
        "\n",
        "# Heuristic filter for active groups\n",
        "if 'deletedDate' in groups_df.columns:\n",
        "    groups_active_df = groups_df.filter(F.col('deletedDate').isNull())\n",
        "elif 'deletedDateTime' in groups_df.columns:\n",
        "    groups_active_df = groups_df.filter(F.col('deletedDateTime').isNull())\n",
        "elif 'state' in groups_df.columns:\n",
        "    groups_active_df = groups_df.filter(F.col('state') == 'active')\n",
        "else:\n",
        "    groups_active_df = groups_df\n",
        "\n",
        "group_ids_df = groups_active_df.select(F.col('id').alias('group_id')).distinct()\n",
        "print(f\"Number of distinct groups to process (estimate): {group_ids_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Graph API fetching helper (pagination + retries) and distributed fetch\n",
        "\n",
        "Define a robust `get_group_owners` function (handles pagination `@odata.nextLink` and exponential backoff for HTTP 429/5xx). Use `mapInPandas` to run across the cluster in parallel and return a flattened DataFrame with schema: `group_id`, `owner_id`, `owner_displayName`, `owner_userPrincipalName`.\n",
        "\n",
        "Notes:\n",
        "- For very large numbers of groups, tune the partition count in `repartition(...)` below according to cluster size.\n",
        "- If Graph API throttles, our function will back off and retry up to the configured maximum attempts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "import pandas as pd\n",
        "\n",
        "output_schema = StructType([\n",
        "    StructField('group_id', StringType(), True),\n",
        "    StructField('owner_id', StringType(), True),\n",
        "    StructField('owner_displayName', StringType(), True),\n",
        "    StructField('owner_userPrincipalName', StringType(), True),\n",
        "])\n",
        "\n",
        "def fetch_owners_map(iterator):\n",
        "    import requests, time\n",
        "    access_token = access_token_b.value\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {access_token}',\n",
        "        'Accept': 'application/json'\n",
        "    }\n",
        "\n",
        "    def get_group_owners(group_id):\n",
        "        owners = []\n",
        "        url = f'https://graph.microsoft.com/v1.0/groups/{group_id}/owners?$select=id,displayName,userPrincipalName'\n",
        "        max_retries = 5\n",
        "        backoff_base = 1.0\n",
        "        while url:\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    resp = requests.get(url, headers=headers, timeout=30)\n",
        "                except requests.exceptions.RequestException:\n",
        "                    sleep_sec = backoff_base * (2 ** attempt)\n",
        "                    time.sleep(sleep_sec)\n",
        "                    continue\n",
        "\n",
        "                if resp.status_code == 200:\n",
        "                    data = resp.json()\n",
        "                    if 'value' in data:\n",
        "                        for o in data['value']:\n",
        "                            owners.append({\n",
        "                                'id': o.get('id'),\n",
        "                                'displayName': o.get('displayName'),\n",
        "                                'userPrincipalName': o.get('userPrincipalName')\n",
        "                            })\n",
        "                    url = data.get('@odata.nextLink')\n",
        "                    break\n",
        "\n",
        "                elif resp.status_code in (429, 503):\n",
        "                    retry_after = None\n",
        "                    try:\n",
        "                        retry_after = int(resp.headers.get('Retry-After', 0))\n",
        "                    except Exception:\n",
        "                        retry_after = None\n",
        "                    if retry_after and retry_after > 0:\n",
        "                        time.sleep(retry_after)\n",
        "                    else:\n",
        "                        sleep_sec = backoff_base * (2 ** attempt)\n",
        "                        time.sleep(sleep_sec)\n",
        "                    continue\n",
        "\n",
        "                elif 500 <= resp.status_code < 600:\n",
        "                    sleep_sec = backoff_base * (2 ** attempt)\n",
        "                    time.sleep(sleep_sec)\n",
        "                    continue\n",
        "\n",
        "                else:\n",
        "                    # Non-retriable client error - log and return empty list\n",
        "                    try:\n",
        "                        _err = resp.json()\n",
        "                    except Exception:\n",
        "                        _err = resp.text\n",
        "                    print(f\"Non-retriable HTTP error for group {group_id}: {resp.status_code} - {_err}\")\n",
        "                    return []\n",
        "            else:\n",
        "                print(f\"Exceeded retries fetching owners for group {group_id}. Returning partial results.\")\n",
        "                return owners\n",
        "        return owners\n",
        "\n",
        "    for pdf in iterator:\n",
        "        out_rows = []\n",
        "        for gid in pdf['group_id'].tolist():\n",
        "            try:\n",
        "                owners = get_group_owners(gid)\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching owners for group {gid}: {e}\")\n",
        "                owners = []\n",
        "            if not owners:\n",
        "                continue\n",
        "            for o in owners:\n",
        "                out_rows.append({\n",
        "                    'group_id': gid,\n",
        "                    'owner_id': o.get('id'),\n",
        "                    'owner_displayName': o.get('displayName'),\n",
        "                    'owner_userPrincipalName': o.get('userPrincipalName')\n",
        "                })\n",
        "        if len(out_rows) == 0:\n",
        "            yield pd.DataFrame(columns=['group_id','owner_id','owner_displayName','owner_userPrincipalName'])\n",
        "        else:\n",
        "            yield pd.DataFrame(out_rows)\n",
        "\n",
        "# Adjust repartition number according to cluster size and number of groups\n",
        "owners_df = group_ids_df.repartition(200).mapInPandas(fetch_owners_map, schema=output_schema)\n",
        "owners_df = owners_df.persist()\n",
        "print(f\"Fetched owner rows (estimate): {owners_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Prepare SCD Type 2 source\n",
        "\n",
        "We will create a source that contains:\n",
        "- All owners fetched from Graph (flagged `present = true`).\n",
        "- All currently active owners from the target table that are _not_ present in today's snapshot (flagged `present = false`).\n",
        "This union allows performing all SCD transitions (insert new, close removed, handle attribute changes) using a single atomic `MERGE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Add present flag to owners fetched this run\n",
        "owners_present_df = owners_df.withColumn('present', lit(True))\n",
        "\n",
        "# If the target delta table doesn't exist yet, create it from the current snapshot\n",
        "def delta_table_exists(path):\n",
        "    try:\n",
        "        return DeltaTable.isDeltaTable(spark, path)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "if not delta_table_exists(target_owner_table_path):\n",
        "    print(f\"Target Delta table not found at {target_owner_table_path}. Creating initial table...\")\n",
        "    initial_df = owners_present_df.withColumn('start_date', F.to_timestamp(F.lit(run_timestamp_str))) \\\n",
        "                                     .withColumn('end_date', F.lit(None).cast(TimestampType())) \\\n",
        "                                     .withColumn('is_current', lit(True))\n",
        "    initial_df.write.format('delta').mode('overwrite').save(target_owner_table_path)\n",
        "    print(\"Initial target table created.\")\n",
        "else:\n",
        "    print(f\"Target Delta table exists at {target_owner_table_path}. Proceeding to SCD MERGE flow.\")\n",
        "\n",
        "target_df = spark.read.format('delta').load(target_owner_table_path)\n",
        "target_current_df = target_df.filter(F.col('is_current') == True).select(\n",
        "    F.col('group_id'), F.col('owner_id'), F.col('owner_displayName'), F.col('owner_userPrincipalName'), F.col('start_date'), F.col('end_date')\n",
        ")\n",
        "# owners currently active in target but missing from source = removed owners\n",
        "removed_owners_df = target_current_df.join(\n",
        "    owners_present_df.select('group_id', 'owner_id'),\n",
        "    on=['group_id', 'owner_id'],\n",
        "    how='left_anti'\n",
        ").select('group_id', 'owner_id', 'owner_displayName', 'owner_userPrincipalName')\n",
        "removed_owners_df = removed_owners_df.withColumn('present', lit(False))\n",
        "\n",
        "scd_source_df = owners_present_df.unionByName(removed_owners_df)\n",
        "scd_source_df = scd_source_df.persist()\n",
        "scd_source_view = 'vw_scd_owners_source'\n",
        "scd_source_df.createOrReplaceTempView(scd_source_view)\n",
        "print(f\"SCD source rows (present + removed): {scd_source_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) SCD Type 2 MERGE (single atomic operation)\n",
        "\n",
        "This single `MERGE` handles:\n",
        "1. **Removed owners**: `present = false` → close existing active record by setting `is_current=false` and `end_date=run_timestamp`.\n",
        "2. **Attribute changes**: existing active record where attributes differ → close the old record by setting `is_current=false` and `end_date=run_timestamp`.\n",
        "3. **New owners**: insert new record with `is_current=true` and `start_date=run_timestamp`.\n",
        "\n",
        "Because we use `run_timestamp_str` consistently, re-running the same run will be idempotent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merge_sql = f\"\"\"\n",
        "MERGE INTO delta.`{target_owner_table_path}` AS tgt\n",
        "USING (SELECT group_id, owner_id, owner_displayName, owner_userPrincipalName, present FROM {scd_source_view}) AS src\n",
        "ON tgt.group_id = src.group_id AND tgt.owner_id = src.owner_id\n",
        "\n",
        "-- 1) Close out records that are currently active in target but missing from today's snapshot\n",
        "WHEN MATCHED AND tgt.is_current = true AND src.present = false\n",
        "  THEN UPDATE SET tgt.is_current = false, tgt.end_date = TIMESTAMP('{run_timestamp_str}')\n",
        "\n",
        "-- 2) Handle attribute changes for an owner (close existing record)\n",
        "WHEN MATCHED AND tgt.is_current = true AND src.present = true AND (\n",
        "      (coalesce(tgt.owner_displayName, '') <> coalesce(src.owner_displayName, ''))\n",
        "   OR (coalesce(tgt.owner_userPrincipalName, '') <> coalesce(src.owner_userPrincipalName, ''))\n",
        ")\n",
        "  THEN UPDATE SET tgt.is_current = false, tgt.end_date = TIMESTAMP('{run_timestamp_str}')\n",
        "\n",
        "-- 3) Insert new records (new owners and also inserts after attribute-change closures)\n",
        "WHEN NOT MATCHED AND src.present = true\n",
        "  THEN INSERT (group_id, owner_id, owner_displayName, owner_userPrincipalName, start_date, end_date, is_current)\n",
        "       VALUES (src.group_id, src.owner_id, src.owner_displayName, src.owner_userPrincipalName, TIMESTAMP('{run_timestamp_str}'), NULL, true)\n",
        "\"\"\"\n",
        "\n",
        "print(\"Executing MERGE ...\")\n",
        "spark.sql(merge_sql)\n",
        "print(\"MERGE completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Post-merge maintenance: OPTIMIZE + ZORDER\n",
        "\n",
        "Run `OPTIMIZE` and `ZORDER` to keep the Delta table performant for queries. If your runtime does not support `OPTIMIZE`, the command may error — this is safe to ignore or run manually later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    print(\"Running OPTIMIZE and ZORDER... This may take time depending on table size.\")\n",
        "    spark.sql(f\"OPTIMIZE delta.`{target_owner_table_path}` ZORDER BY (group_id)\")\n",
        "    print(\"OPTIMIZE completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"OPTIMIZE failed or not supported on this runtime: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Validation / Quick checks and example queries\n",
        "\n",
        "Run a few quick checks and then example queries you can use to validate the pipeline (counts, current owners, history for a group)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quick_counts():\n",
        "    tgt = spark.read.format('delta').load(target_owner_table_path)\n",
        "    total = tgt.count()\n",
        "    current = tgt.filter(F.col('is_current') == True).count()\n",
        "    closed = tgt.filter(F.col('is_current') == False).count()\n",
        "    print(f\"Target total rows: {total}, current: {current}, closed: {closed}\")\n",
        "\n",
        "quick_counts()\n",
        "\n",
        "# Example validation queries (uncomment to run):\n",
        "# 1) Show some currently active owners\n",
        "# display(spark.read.format('delta').load(target_owner_table_path).filter('is_current = true').limit(50))\n",
        "\n",
        "# 2) Show history of a specific group ID (replace GROUP_ID)\n",
        "# display(spark.sql(f\"SELECT * FROM delta.`{target_owner_table_path}` WHERE group_id = 'GROUP_ID' ORDER BY start_date DESC\"))\n",
        "\n",
        "# 3) Show Delta history for the table (useful for debugging)\n",
        "# display(spark.sql(f\"DESCRIBE HISTORY delta.`{target_owner_table_path}`\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Operational notes / recommended production enhancements\n",
        "\n",
        "- Replace `print` statements with structured logging (send to Azure Monitor / Log Analytics).  \n",
        "- Consider chunking the `group_ids_df` into batches if you have thousands of groups and the Graph API throttles heavily.  \n",
        "- Monitor Delta table sizes and optimize file compaction/retention according to your operational needs.  \n",
        "- Ensure the AAD app has the minimal required permissions (e.g., `Group.Read.All`).  \n",
        "- Parameterize and schedule the notebook as a Databricks job, and set retry/backoff policies on the job level for failover handling."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
